// Copyright (c) 2021 IBM Corporation and others.
// Licensed under Creative Commons Attribution-NoDerivatives
// 4.0 International (CC BY-ND 4.0)
//   https://creativecommons.org/licenses/by-nd/4.0/
//
// Contributors:
//     IBM Corporation
//
:projectid: crc-openshift
:page-layout: guide-multipane
:page-duration: 45 minutes 
:page-releasedate: 2021-05-30
:page-description: Explore how to deploy microservices to a local OpenShift cluster running with CodeReady Containers
:page-tags: ['Kubernetes', 'Docker', 'Cloud'] 
:page-permalink: /guides/{projectid}
:page-related-guides: ['kubernetes-intro', 'kubernetes-microprofile-config', 'kubernetes-microprofile-health', 'istio-intro', 'cloud-openshift-operator'] 
:common-includes: https://raw.githubusercontent.com/OpenLiberty/guides-common/master
:source-highlighter: prettify
:page-seo-title: Deploying Java microservices to Red Hat OpenShift using CodeReady Containers
:page-seo-description: A getting started tutorial with examples on how to deploy Java microservices to a Kubernetes cluster on OpenShift running locally with CodeReady Containers using OpenShift Container Registry (OCR). 
:guide-author: Open Liberty
= Deploying microservices to OpenShift using CodeReady Containers

[.hidden]
NOTE: This repository contains the guide documentation source. To view the guide in published form, view it on the https://openliberty.io/guides/{projectid}.html[Open Liberty website^].

Explore how to deploy microservices to a local OpenShift cluster running with CodeReady Containers

:kube: Kubernetes
:hashtag: #
:win: WINDOWS
:mac: MAC
:linux: LINUX
:system-api: http://[hostname]:31000/system/properties
:inventory-api: http://[hostname]:32000/inventory/systems

// =================================================================================================
// Introduction
// =================================================================================================

== What you'll learn 

You'll learn how to deploy two microservices in Open Liberty containers to an OpenShift 4 cluster 
that is running locally on your computer by using CodeReady Containers.
To learn how to deploy to an OpenShift 4 cluster by using operators, see the 
https://openliberty.io/guides/cloud-openshift-operator.html[Deploying microservices to OpenShift by using Kubernetes Operators^] guide. 

Different cloud-based solutions are available for running your {kube} workloads. 
With a cloud-based infrastructure, you can focus on developing your microservices without worrying 
about low-level infrastructure details for deployment. 
By using the cloud, you can easily scale and manage your microservices in a high-availability setup.

{kube} is an open source container orchestrator that automates many tasks that are involved in deploying, 
managing, and scaling containerized applications. 
To learn more about Kubernetes, 
check out the https://openliberty.io/guides/kubernetes-intro.html[Deploying microservices to Kubernetes^] guide.

https://access.redhat.com/documentation/en-us/red_hat_codeready_containers/1.23/html/getting_started_guide/index[Red Hat CodeReady Containers^] (CRC) is a tool that you can use to quickly build and run
a minimal OpenShift 4 cluster on your local computer. CRC simplifies setup 
and testing while providing all of tools that are needed to develop container-based
applications. 

The two microservices that you'll deploy are called `system` and `inventory`. 
The `system` microservice returns the JVM system properties of the running container. 
It also returns the pod name in the HTTP header, which makes pod replicas more distinguishable from each other.
The `inventory` microservice adds the properties from the `system` microservice to the inventory. 
This process demonstrates how communication can be established between pods inside a cluster.

// =================================================================================================
// Prerequisites
// =================================================================================================

== Additional prerequisites

Before you begin, the following tools need to be installed:

* *Docker:* You need a containerization software for building containers. 
Kubernetes supports various container types, but you'll use Docker in this guide. 
For installation instructions, refer to the official https://docs.docker.com/get-docker/[Docker documentation^].

* *CodeReady Containers:* You need to install CRC to run
OpenShift 4 locally on your computer. For installation instructions, refer to the
official https://access.redhat.com/documentation/en-us/red_hat_codeready_containers/1.23/html/getting_started_guide/installation_gsg[CodeReady Containers documentation].   

// =================================================================================================
// Getting Started
// =================================================================================================

[role=command]
include::{common-includes}/gitclone.adoc[]

// no "try what you'll build" section in this guide since it would be too long due to all setup the user will have to do.

// =================================================================================================
// Starting CodeReady Containers
// =================================================================================================

== Starting CodeReady Containers

=== Setting up CodeReady Containers

Run the following command to set up your host machine for CodeReady Containers:

[role=command]
```
crc setup
```

=== Starting the virtual machine

Next, run the following command to start the CodeReady Containers virtual machine and OpenShift cluster:

[role=command]
```
crc start
```

When prompted, supply your user pull secret. The pull secret can be found on the
page where you downloaded the
https://cloud.redhat.com/openshift/create/local[latest release of CodeReady Containers].

If the cluster starts successfully, you can see output similar to the following example:

[role="no_copy"]
----
Started the OpenShift cluster.

The server is accessible via web console at:
  https://console-openshift-console.apps-crc.testing

Log in as administrator:
  Username: kubeadmin
  Password: jPvDv-jgRZB-qhYP4-Hmkmj

Log in as user:
  Username: developer
  Password: developer
----

Save this output as it might be required later in this guide. 

=== Logging in to the cluster 

To interact with the OpenShift cluster, you need to use the `oc` commands.
CRC already includes the `oc` binary. 
To use the `oc` commands, run the following command to see instructions on how to configure your PATH:

[role=command]
```
crc oc-env
```

The resulting output differs based on your OS and environment, but you should get an output similar to the following:

include::{common-includes}/os-tabs.adoc[]
[.tab_content.linux_section]
--
[role="no_copy"]
----
export PATH="/Users/developer/.bin/oc:$PATH"
# Run this command to configure your shell session:
# eval $(crc oc-env)
----

Run the command that is specified in the output to configure your shell session:

[role=command]
```
eval $(crc oc-env)
```
--

[.tab_content.mac_section]
--
[role="no_copy"]
----
export PATH="/Users/developer/.bin/oc:$PATH"
# Run this command to configure your shell session:
# eval $(crc oc-env)
----

Run the appropriate command specified in the output to configure your environment:

[role=command]
```
eval $(crc oc-env)
```
--

[.tab_content.windows_section] 
--
[role="no_copy"]
----
SET PATH=C:\Users\developer\.crc\bin\oc;%PATH%
REM Run this command to configure your shell:
REM     @FOR /f "tokens=*" %i IN ('crc oc-env') DO @call %i
----

Run the appropriate command specified in the output to configure your environment:

[role=command]
```
@FOR /f "tokens=*" %i IN ('crc oc-env') DO @call %i
```
--

Run the following command to log in and gain access to your OpenShift cluster:

[role=command]
```
oc login -u developer https://api.crc.testing:6443
```

If prompted, enter the `developer` password from the output for the `crc start` command that you ran in the _Starting the virtual machine_ section.
Next, create a new OpenShift project with the name `my-project` by running the following command:

[role=command]
```
oc new-project my-project
```

You're now ready to build and deploy microservices.

// =================================================================================================
// Deploying microservices to OpenShift
// =================================================================================================

== Deploying microservices to OpenShift

In this section, you'll learn how to deploy two microservices in Open Liberty containers to a {kube}
cluster on OpenShift. You'll build and containerize the `system` and `inventory` microservices, 
push them to a container registry, and then deploy them to your {kube} cluster. 

// =================================================================================================
// Building and containerizing the microservices
// =================================================================================================

=== Building and containerizing the microservices

The first step of deploying to {kube} is to build your microservices and containerize them.

The starting Java project, which is located in the start directory, is a multi-module Maven project. 
It's made up of the `system` and `inventory` microservices. 
Each microservice is located in its own directory: `start/system` for the `system` microservice and `start/inventory` for the `inventory` microservice. 
Each of these directories contains a Dockerfile, which is necessary for building the Docker images. 
See the https://openliberty.io/guides/containerize.html[Containerizing microservices^] 
guide if you're unfamiliar with Dockerfiles.

If you're familiar with Maven and Docker, you might be tempted to run a Maven build first and then
use the `.war` file to build a Docker image. However, these projects are set up so that this process is automated 
as a part of a single Maven build.

Go to the `start` directory and build these microservices by running the following commands:

[role=command]
```
cd start
mvn package
```

include::{common-includes}/ol-full-docker-pull.adoc[]

Next, run the `docker build` commands to build container images for your application:
[role='command']
```
docker build -t system:1.0-SNAPSHOT system/.
docker build -t inventory:1.0-SNAPSHOT inventory/.
```

During the build, you see various Docker messages that describe what images are being downloaded and
built. When the build finishes, run the following command to list all local Docker images:

[role=command]
```
docker images
```

Verify that the `system:1.0-SNAPSHOT` and `inventory:1.0-SNAPSHOT` images are listed among them, for example:

[role="no_copy"]
----
REPOSITORY                    TAG
system                        1.0-SNAPSHOT
inventory                     1.0-SNAPSHOT
openliberty/open-liberty      kernel-java8-openj9-ubi
----

If you don't see the `system:1.0-SNAPSHOT` and `inventory:1.0-SNAPSHOT` images, check the Maven
build log for any potential errors.

// =================================================================================================
// Pushing the images to OpenShift's internal registry
// =================================================================================================

=== Pushing the images to OpenShift's internal registry

In order to run the microservices on the cluster, 
you need to push the microservice images to a container image registry. 
You'll use OpenShift Container Registry (OCR), which is the OpenShift integrated container image registry. 
After your images are pushed to the registry, you can use them in the pods that you create later in the guide.

Run the login command to authenticate your Docker client to your OCR.:

[role=command]
```
oc registry login --skip-check
```

To use the OpenShift registry domain, add it to the list of insecure registries for your Docker Engine.

include::{common-includes}/os-tabs.adoc[]

[.tab_content.mac_section]
--
Click the Docker icon in your menu bar, select *Preferences*, and then click *Docker Engine*. 
If a JSON object already exists, add the following key/value pair to it:

[role="command"]
----
"insecure-registries": ["default-route-openshift-image-registry.apps-crc.testing"]
----

If no JSON object exists, add the following object: 

[role="command"]
----
{
  "insecure-registries": ["default-route-openshift-image-registry.apps-crc.testing"]
}
----

Next, press the *Apply & Restart* button for the changes to take effect.
--

[.tab_content.windows_section]
--
Click the Docker icon in your system tray, select *Settings*, and then click *Docker Engine*. 
If a JSON object already exists, add the following key/value pair to it:

[role="command"]
----
"insecure-registries": ["default-route-openshift-image-registry.apps-crc.testing"]
----

If there's no existing JSON object, instead add the following object: 

[role="command"]
----
{
  "insecure-registries": ["default-route-openshift-image-registry.apps-crc.testing"]
}
----

Next, press the *Apply & Restart* button for the changes to take effect. 
--

[.tab_content.linux_section]
--

Open the `/etc/docker/daemon.json` file or create it if it doesn't already exist. 
If the file already contains a JSON object, add the following key/value pair: 

[role="command"]
----
"insecure-registries": ["default-route-openshift-image-registry.apps-crc.testing"]
----

If the file doesn't already contain a JSON object, instead add the following object:

[role="command"]
----
{
  "insecure-registries": ["default-route-openshift-image-registry.apps-crc.testing"]
}
----

Next, restart Docker for the changes to take effect.
--

To learn more about testing insecure registries, 
check out the official https://docs.docker.com/registry/insecure/[Docker documentation]. 

You can store your Docker credentials in a custom external credential store, 
which is more secure than using a Docker configuration file. 
If you're using a custom credential store for securing your registry credentials, 
or if you're unsure where your credentials are stored, use the following command:

include::{common-includes}/os-tabs.adoc[]

[.tab_content.mac_section.linux_section]
--
[role=command]
```
echo $(oc whoami -t) | docker login -u developer --password-stdin $(oc registry info)
```
--

[.tab_content.windows_section]
--
Because the Windows command prompt doesnâ€™t support the command substitution that is displayed for Mac and Linux, 
run the following commands: 
[role=command]
```
oc whoami
oc whoami -t
oc registry info
```

Replace the square brackets in the following `docker login` command with the results from the previous commands:
[role=command]
```
docker login -u [oc whoami] -p [oc whoami -t] [oc registry info]
```
--

This command authenticates your credentials against the internal registry so that
you're able to push and pull images.

You can view the registry address by running the following command:

[role=command]
```
oc registry info
```

The output is similar to the following:

[role="no_copy"]
----
default-route-openshift-image-registry.apps-crc.testing
----

Ensure that you're logged in to OpenShift and the registry, and run the following commands to tag your applications:

include::{common-includes}/os-tabs.adoc[]

[.tab_content.mac_section.linux_section]
--
[role=command]
```
docker tag system:1.0-SNAPSHOT $(oc registry info)/$(oc project -q)/system:1.0-SNAPSHOT
docker tag inventory:1.0-SNAPSHOT $(oc registry info)/$(oc project -q)/inventory:1.0-SNAPSHOT
```
--

[.tab_content.windows_section]
--
Run the following commands:   
[role=command]
```
oc registry info
oc project -q
```

Replace the square brackets in the following `docker tag` commands with the results from the previous commands:
[role=command]
```
docker tag system:1.0-SNAPSHOT [oc registry info]/[oc project -q]/system:1.0-SNAPSHOT
docker tag inventory:1.0-SNAPSHOT [oc registry info]/[oc project -q]/inventory:1.0-SNAPSHOT
```
--

Finally, push your images to the registry:

include::{common-includes}/os-tabs.adoc[]

[.tab_content.mac_section.linux_section]
--
[role=command]
```
docker push $(oc registry info)/$(oc project -q)/system:1.0-SNAPSHOT
docker push $(oc registry info)/$(oc project -q)/inventory:1.0-SNAPSHOT
```
--

[.tab_content.windows_section]
--
Run the following commands:
[role=command]
```
oc registry info
oc project -q
```

Replace the square brackets in the following `docker push` commands with the results from the previous commands:
[role=command]
```
docker push [oc registry info]/[oc project -q]/system:1.0-SNAPSHOT
docker push [oc registry info]/[oc project -q]/inventory:1.0-SNAPSHOT
```
--

After you push the images, run the following command to list the images that you pushed to the internal OCR:
[role=command]
```
oc get imagestream
```

Verify that the `system` and `inventory` images are listed among them, for example:
[role="no_copy"]
----
NAME        IMAGE REPOSITORY                                                                                     TAGS           UPDATED
inventory   default-route-openshift-image-registry.apps.us-west-1.starter.openshift-online.com/guide/inventory   1.0-SNAPSHOT   3 seconds ago
system      default-route-openshift-image-registry.apps.us-west-1.starter.openshift-online.com/guide/system      1.0-SNAPSHOT   17 seconds ago
----

// =================================================================================================
// Deploying the microservices
// =================================================================================================

=== Deploying the microservices

Now that your container images are built, deploy them by using a Kubernetes object configuration file.

{kube} objects can be configured in a YAML file that contains a description of all your 
deployments, services, or any other objects that you want to deploy. 
All objects can also be deleted from the cluster by using the same YAML file that you used to deploy them. 
If you're interested in learning more about using and configuring Kubernetes clusters, check out the 
https://openliberty.io/guides/kubernetes-intro.html[Deploying microservices to Kubernetes^] guide.

kubernetes.yaml
[source, yaml, linenums, role="code_column"]
----
include::finish/kubernetes.yaml[]
----

[role="code_command hotspot", subs="quotes"]
----
#Create the `kubernetes.yaml` file.#
`kubernetes.yaml`
----

[role="edit_command_text"]
The [hotspot=systemImage hotspot=inventoryImage]`image` is the name and tag of the container image that you want 
to use for the container. The image address is the OCR address that you logged in to.

Run the following commands to deploy the objects as defined in kubernetes.yaml file:
[role='command']
```
oc apply -f kubernetes.yaml
```

You see an output similar to the following example:

[role="no_copy"]
----
deployment.apps/system-deployment created
deployment.apps/inventory-deployment created
service/system-service created
service/inventory-service created
route.route.openshift.io/system-route created
route.route.openshift.io/inventory-route created
----

When the apps are deployed, run the following command to check the status of your pods:
[role='command']
```
oc get pods
```

If all the pods are healthy and running, you see an output similar to the following example:
[source, role="no_copy"]
----
NAME                                    READY     STATUS    RESTARTS   AGE
system-deployment-6bd97d9bf6-4ccds      1/1       Running   0          15s
inventory-deployment-645767664f-nbtd9   1/1       Running   0          15s
----

// =================================================================================================
// Making requests to the microservices
// =================================================================================================

== Making requests to the microservices

Routes are used to access the services and the application. A route in OpenShift exposes a service at 
a hostname such as `www.your-web-app.com` so external users can access the application. 

kubernetes.yaml
[source, yaml, linenums, role="code_column"]
----
include::finish/kubernetes.yaml[]
----

Both the [hotspot=systemRoute]`system` and [hotspot=inventoryRoute]`inventory` routes are configured in the 
[hotspot]`kubernetes.yaml` file, and running the `oc apply -f kubernetes.yaml` command exposed both services.

Your microservices can now be accessed through the hostnames that you can find by running the following command:

[role='command']
```
oc get routes
```

The routes can also be found in the web console.
The web console URL is similar to `console-openshift-console.apps-crc.testing` and was given as part of
the output of the `crc start` command at the start of the guide. 
When you access the web console, log in using the administrator account credentials. 
After logging in, you can view the routes by navigating to the `Networking > Routes` page. 
Hostnames are in the `Location` columnn in the `inventory-route-my-project.apps-crc.testing` format. 
Ensure that you're in your project, not the `default` project, 
which is shown in the `Project:` field for the console.

Enter the following URLs into your web browser to access your microservices. Substitute the hostnames that you obtained from the `oc get routes` command
for the `system` and `inventory` services:

* `http://[system-hostname]/system/properties/`
* `http://[inventory-hostname]/inventory/systems`

In the first URL, you see the system properties of the container JVM in JSON format. 
The second URL returns an empty list, which is expected because 
no system properties are stored in the inventory yet.

Point your browser to the `http://[inventory-hostname]/inventory/systems/[system-hostname]` URL. 
When you go to this URL, the system properties that are taken from the `system` service are 
automatically stored in the inventory. 
Revisit the `http://[inventory-hostname]/inventory/systems` URL and you see a new entry.

// =================================================================================================
// Testing the microservices
// =================================================================================================

== Testing the microservices

pom.xml
[source, xml, linenums, role='code_column']
----
include::finish/inventory/pom.xml[]
----

A few tests are included for you to test the basic functions of the microservices. 
If a test failure occurs, then you might have introduced a bug into the code. 
To run the tests, wait for all pods to be in the ready state before you proceed further. 
The default properties that are defined in the [hotspot]`pom.xml` file are:

[cols="15, 100", options="header"]
|===
| *Property*                                            | *Description*
| [hotspot=systemIP]`system.ip`                         | IP or hostname of the `system-service` {kube} Service
| [hotspot=inventoryIP]`inventory.ip`                   | IP or hostname of the `inventory-service` {kube} Service
|===

Use the following command to run the integration tests against your cluster:

[role=command]
```
mvn verify \ 
-Dsystem.ip=system-route-my-project.apps-crc.testing \
-Dinventory.ip=inventory-route-my-project.apps-crc.testing
```
* Replace the `system.ip` parameter with the appropriate hostname to access your system microservice.
* Replace the `inventory.ip` parameter with the appropriate hostname to access your inventory microservice.

If the tests pass, you see an output for each service similar to the following examples:

[source, role="no_copy"]
----
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running it.io.openliberty.guides.system.SystemEndpointIT
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.673 sec - in it.io.openliberty.guides.system.SystemEndpointIT

Results:

Tests run: 2, Failures: 0, Errors: 0, Skipped: 0
----

[source, role="no_copy"]
----
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running it.io.openliberty.guides.inventory.InventoryEndpointIT
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.222 sec - in it.io.openliberty.guides.inventory.InventoryEndpointIT

Results:

Tests run: 4, Failures: 0, Errors: 0, Skipped: 0
----

// =================================================================================================
// Tearing down the environment
// =================================================================================================

== Tearing down the environment

When you no longer need your deployed microservices, you can delete the {kube} deployments, services, and routes 
by running the following command:

[role='command']
```
oc delete -f kubernetes.yaml
```

To delete the pushed images, run the following commands:

[role='command']
```
oc delete imagestream/inventory
oc delete imagestream/system
```

Next, you can delete the project by running the following command:

[role='command']
```
oc delete project my-project
```

Finally, you can stop and delete the CRC virtual machine by running the following commands:

[role='command']
```
crc stop
crc delete
```

// =================================================================================================
// finish
// =================================================================================================

== Great work! You're done!

You just deployed two microservices running in Open Liberty to OpenShift using CodeReady Containers. You also 
learned how to use `oc` to deploy your microservices on a {kube} cluster.

// Multipane
include::{common-includes}/attribution.adoc[subs="attributes"]

// DO NO CREATE ANYMORE SECTIONS AT THIS POINT
// Related guides will be added in automatically here if you included them in ":page-related-guides"
